{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subject-Verb-RestofSentence preprocesser\n",
    "\n",
    "The purpose of this text preprocessor is to segment sentences to improve performance of a LSTM model trained to determine semantic text similarity between 2 sentences.\n",
    "\n",
    "Three NLP libraries (Stanford CoreNLP, SpaCy and NLTK) are used to generate parts of speech (postag) for each word of a sentence and best chosen from examining where the postags differ.\n",
    "\n",
    "The first verb block is used to segment each sentence into 3 parts: (1) part of sentence before verb block, (2) first verb block, (3) rest of sentence. First and third part will switch places if a passive voice sentence is confidently identified.\n",
    "\n",
    "The 3 parts of the sentences are then aligned to determine semantic similarity for each sentence pair using a bidrectional LSTM model trained on the microsoft short phrase similarity dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# keep postags as two list of strings : words and postags\n",
    "# create 3 sets of postag for entire dataset \n",
    "# check difference in number of words between 3 sets\n",
    "# and do svo-split . . .then check difference in verb block, which should determine the split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy_nlp = spacy.load('en')               # larger alternative 'en_core_web_lg'\n",
    "spacytkn = lambda sentstr : spacy_nlp(sentstr)\n",
    "# takes a string sentence and returns list of tokens which are \n",
    "# class with postag in token.pos_, token.text etc\n",
    "\n",
    "import nltk\n",
    "\n",
    "from pycorenlp import StanfordCoreNLP\n",
    "scnlp_nlp = StanfordCoreNLP('http://localhost:9000')\n",
    "scnlptkn  = lambda sentstr : scnlp_nlp.annotate(sentstr, properties = {\n",
    "                                        'annotators': 'tokenize,ssplit,pos,depparse,parse',\n",
    "                                        'outputFormat': 'json' })['sentences'][0]\n",
    "scnlp_tree   = lambda sentstr : scnlptkn(sentstr)['parse']+'\\n'\n",
    "scnlp_tree_strlist   = lambda strlist : [scnlp_tree(sentstr)   for sentstr in strlist]\n",
    "\n",
    "# >> note : \n",
    "# >> scnlp_nlp.annotate takes a string input and gives dictionary output\n",
    "# >> scnlp_tree extracts dictionary value (which is a string) for key = 'parse' and add '\\n'\n",
    "\n",
    "import pprint\n",
    "prettyprinter = pprint.PrettyPrinter(indent=1)\n",
    "oprint = lambda inobj : prettyprinter.pprint(inobj)\n",
    "\n",
    "bprint = lambda s : print(bytes(s,encoding='utf-8'))\n",
    "ex_utf=[i for i in range(0,32,1)] + [i for i in range(127,255,1)] # define non utf characters\n",
    "notutf8 = lambda s : any(c in ex_utf for c in bytes(s, encoding='utf-8'))  \n",
    "flatten = lambda list_of_list : [val for sublist in list_of_list for val in sublist]\n",
    "\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Proj_Me1/data/sts_source/sts-train.txt records count: 5749\n",
      "../Proj_Me1/data/sts_source/sts-dev.txt records count: 1500\n",
      "../Proj_Me1/data/sts_source/sts-test.txt records count: 1379\n",
      "Total data samples: 8628\n",
      "b\"Treasury's long-range plan is to provide retail buyers of all Treasury securities the ability to manage their holdings online in a single account.\"\n"
     ]
    }
   ],
   "source": [
    "fname_sts_trn = '../Proj_Me1/data/sts_source/sts-train.txt'\n",
    "fname_sts_val = '../Proj_Me1/data/sts_source/sts-dev.txt'\n",
    "fname_sts_tes = '../Proj_Me1/data/sts_source/sts-test.txt'\n",
    "\n",
    "def read_sts(fname):\n",
    "\n",
    "    f = open(fname, 'r', encoding='utf-8')\n",
    "\n",
    "    i = 0\n",
    "    scores = []\n",
    "    sentences1 = []\n",
    "    sentences2 = []\n",
    "    while True:\n",
    "        l = f.readline()  # read 1 line\n",
    "        if not l:\n",
    "            break\n",
    "\n",
    "        data_fields = l.split(\"\\t\")\n",
    "        scores.append(float(data_fields[4]))\n",
    "        sentences1.append(data_fields[5])\n",
    "        sentences2.append(data_fields[6])\n",
    "        i = i + 1\n",
    "    \n",
    "        \n",
    "    f.close()\n",
    "    print(fname,\"records count:\",i)\n",
    "\n",
    "    return i, scores, sentences1, sentences2\n",
    "\n",
    "# read data files and obtain lists of sentences\n",
    "n_trn, scores_trn, sentences1_trn, sentences2_trn = read_sts(fname_sts_trn)\n",
    "n_val, scores_val, sentences1_val, sentences2_val = read_sts(fname_sts_val)\n",
    "n_tes, scores_tes, sentences1_tes, sentences2_tes = read_sts(fname_sts_tes)\n",
    "\n",
    "n_data = n_trn + n_val + n_tes\n",
    "sent1_all = sentences1_trn + sentences1_val + sentences1_tes\n",
    "sent2_all = sentences2_trn + sentences2_val + sentences2_tes\n",
    "sent_all = sent1_all + sent2_all\n",
    "\n",
    "print(\"Total data samples:\",n_data)\n",
    "\n",
    "## correction in data to prevent nltk from crashing\n",
    "sent_all[2918] = sent_all[2918].replace('\\x12',\"'\")\n",
    "bprint(sent_all[2918])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201611\n"
     ]
    }
   ],
   "source": [
    "# To read and return list of strings, each string is the tree for a sentence\n",
    "# Each tree has been written in multiple lines in the file\n",
    "# Beginning of each tree is the string firstline\n",
    "\n",
    "firstline = '(ROOT\\n'\n",
    "def read_trees(fname):\n",
    "\n",
    "    f = open(fname, 'r', encoding='utf-8')\n",
    "    flines = f.readlines()\n",
    "    f.close()\n",
    "    print(len(flines))\n",
    "\n",
    "    tree = flines[0]  # each tree is a single string with multiple \\n characters\n",
    "    treelist = []     # to store list of tree\n",
    "    assert (tree == firstline)  # check that first line is the string '(ROOT\\n'\n",
    "\n",
    "    for line in flines[1:]:         # read from second line onwards\n",
    "        if line == firstline:       # signifies new tree detected\n",
    "            treelist.append(tree)   # append last tree to treelist\n",
    "            tree = line             # reset tree to first line\n",
    "        else:\n",
    "            tree = tree + line\n",
    "\n",
    "    treelist.append(tree)           # append last tree after exiting loop\n",
    "\n",
    "    return treelist\n",
    "\n",
    "sent_trees = read_trees('./data/scnlp_sent_trees.txt')\n",
    "assert(len(sent_trees) == (n_data*2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to extract (word, postag) list from scnlp parse tree for a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tup(tree, brac_start, brac_end):  # splits substring in tree[brac_start+1:brac_end]\n",
    "                                          # into tupple of 2 strings \n",
    "                                          # e.g. '(VBZ run' into ('VBZ','run')\n",
    "    if brac_end >= brac_start + 3:\n",
    "        sstr = tree[brac_start+1:brac_end]\n",
    "        try:\n",
    "            stup = sstr.split(' ')\n",
    "            tup = (stup[1],stup[0])\n",
    "        except:\n",
    "            tup = (None,sstr)\n",
    "    else:\n",
    "        tup = None     # or (None,None)\n",
    "    return tup\n",
    "\n",
    "def get_next_idx(inlist, idx, item):  # search for location where item next occur in input list\n",
    "    try:\n",
    "        next_idx = inlist[idx+1:].index(item) + idx + 1\n",
    "        endoftree = False\n",
    "    except:\n",
    "        endoftree = True\n",
    "        next_idx = idx    # set to end of list length originally\n",
    "    return endoftree, next_idx\n",
    "        \n",
    "def scnlp_postag(tree):  # extract list of tuples of (word, postag) from tree generated scnlp\n",
    "    treelen = len(tree)\n",
    "    if treelen < 5:\n",
    "        return []\n",
    "    endoftree = False\n",
    "    tuplist = []\n",
    "    next_brac_start = -1\n",
    "    brac_end = -1\n",
    "    while not endoftree:\n",
    "        endoftree, brac_end = get_next_idx(tree,next_brac_start,')') \n",
    "        while not endoftree and next_brac_start < brac_end:\n",
    "            brac_start = next_brac_start\n",
    "            endoftree, next_brac_start = get_next_idx(tree,brac_start,'(')\n",
    "        if brac_start < brac_end and brac_start >= 0:\n",
    "            tup = get_tup(tree, brac_start, brac_end)\n",
    "            #print(tup)\n",
    "            tuplist.append(tup)\n",
    "            brac_start = next_brac_start\n",
    "\n",
    "    return tuplist\n",
    "    \n",
    "\n",
    "# splits list of format [('Eyes', 'NNP'), ('are','VBP') . . ]\n",
    "# into 2 lists : ['Eyes','are' . . ] and ['NNP', 'VBP' . . ]\n",
    "def split_postaglist(postaglist):\n",
    "    return [postag[0] for postag in postaglist], [postag[1] for postag in postaglist]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to split sentence into 3 parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VB_tags = ['VB','VBD','VBG','VBN','VBP','VBZ']\n",
    "verbloc = np.zeros((n_data*2,3,2),dtype=np.int)\n",
    "def find_verbloc(pos):  # find location of first block of verbs, pos is list of pos strings\n",
    "    found = False\n",
    "    poslen = len(postag)\n",
    "    if poslen == 0:\n",
    "        return found, None, None\n",
    "    vstart = 0\n",
    "    vend = 0\n",
    "    while ((vstart+1) < poslen) and (pos[vstart] not in VB_tags):\n",
    "        vstart = vstart + 1\n",
    "    if (vstart < poslen) and (pos[vstart] in VB_tags):\n",
    "        found = True\n",
    "        vend = vstart\n",
    "        while ((vend+1) < poslen) and (pos[vend] in VB_tags):\n",
    "            vend = vend + 1\n",
    "    return found, vstart, vend\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 17256\n"
     ]
    }
   ],
   "source": [
    "# create array of tokenized words and pos\n",
    "tmp_pos = [list()] * n_data * 2\n",
    "sent_pos = [tmp_pos] * 3\n",
    "sent_w = [tmp_pos] * 3\n",
    "\n",
    "#for i in range(n_data*2):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpaCy postag\n",
      "[('*', 'NFP'), ('*', 'NFP'), ('smooches', 'NNS'), ('Ran', 'NNP'), ('*', 'NFP'), ('*', 'NFP'), ('Hey', 'UH'), (',', ','), ('Blue', 'NNP'), ('-', 'HYPH'), ('Eyes', 'NNS'), (',', ','), ('How', 'WRB'), (\"'s\", 'VBZ'), ('things', 'NNS'), ('in', 'IN'), ('London', 'NNP'), ('?', '.'), ('\\n', '')]\n",
      "NLTK postag\n",
      "[('**smooches', 'NNS'), ('Ran**', 'NNP'), ('Hey', 'NNP'), (',', ','), ('Blue-Eyes', 'NNP'), (',', ','), ('How', 'NNP'), (\"'s\", 'POS'), ('things', 'NNS'), ('in', 'IN'), ('London', 'NNP'), ('?', '.')]\n",
      "SCNLP postag\n",
      "[('**', 'SYM'), ('smooches', 'NNS'), ('Ran', 'NN'), ('**', 'SYM'), ('Hey', 'UH'), (',', ','), ('Blue-Eyes', 'NNP'), (',', ','), ('How', 'WRB'), (\"'s\", 'POS'), ('things', 'NNS'), ('in', 'IN'), ('London', 'NNP'), ('?', '.')]\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "n = 10645\n",
    "testsent = sent_all[n]\n",
    "# spacy postag\n",
    "doc = spacy_nlp(testsent)\n",
    "print(\"SpaCy postag\")\n",
    "spacy_postag = [(token.text,token.tag_) for token in doc]\n",
    "print(spacy_postag)\n",
    "\n",
    "# nltk postag\n",
    "stkn = nltk.word_tokenize(testsent)\n",
    "spostag = nltk.pos_tag(stkn)\n",
    "print(\"NLTK postag\")\n",
    "print(spostag)\n",
    "\n",
    "# scnlp postag\n",
    "print(\"SCNLP postag\")\n",
    "tree = sent_trees[n]\n",
    "#print(tree)\n",
    "print(scnlp_postag(tree))\n",
    "print(spacy_postag==scnlp_postag(tree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**smooches Ran** Hey, Blue-Eyes, How's things in London?\n",
      "\n",
      "(ROOT\n",
      "  (X\n",
      "    (X (SYM **))\n",
      "    (NP\n",
      "      (NP (NNS smooches) (NN Ran))\n",
      "      (SBAR\n",
      "        (FRAG\n",
      "          (SBAR\n",
      "            (X (SYM **))\n",
      "            (S\n",
      "              (INTJ (UH Hey))\n",
      "              (, ,)\n",
      "              (NP (NNP Blue-Eyes) (, ,))\n",
      "              (VP (WRB How)\n",
      "                (S\n",
      "                  (NP (POS 's)))\n",
      "                (NP (NNS things))\n",
      "                (PP (IN in)\n",
      "                  (NP (NNP London))))))\n",
      "          (. ?))))))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(sent_all[n])\n",
    "print(tree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('A', 'DT'), ('man', 'NN'), ('is', 'VBZ'), ('spreading', 'VBG'), ('shredded', 'VBN'), ('cheese', 'NN'), ('on', 'IN'), ('a', 'DT'), ('pizza', 'NN'), ('.', '.')]\n",
      "(ROOT\n",
      "  (S\n",
      "    (NP (DT A) (NN man))\n",
      "    (VP (VBZ is)\n",
      "      (VP (VBG spreading)\n",
      "        (NP (JJ shredded) (NN cheese))\n",
      "        (PP (IN on)\n",
      "          (NP (DT a) (NN pizza)))))\n",
      "    (. .)))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testsent = 'A man is spreading shredded cheese on a pizza.'\n",
    "stkn = nltk.word_tokenize(testsent)\n",
    "spostag = nltk.pos_tag(stkn)\n",
    "print(spostag)\n",
    "print(scnlp_parse(testsent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate parse tree using stanford nlpcore library\n",
    "\n",
    "and write to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use stanford corenlp parser to obtain sentence tree so as to obtain noun phrase and verb phrase\n",
    "# Only need to do this once correctly\n",
    "# sentence tree is written to file and can be read back when needed\n",
    "# parsing routine takes 10-15 mimutes for 17k sentences\n",
    "\n",
    "sent_trees = scnlp_tree_strlist(sent_all)\n",
    "\n",
    "# input trees is a list of strings, each string correspond to one sentence tree\n",
    "def write_trees(fname, strlist):    \n",
    "    f = open(fname, 'w', encoding='utf-8')\n",
    "    for s in strlist:\n",
    "        f.write(s)\n",
    "    f.close()\n",
    "\n",
    "assert(len(sent_trees) == (n_data*2))\n",
    "write_trees('./data/scnlp_sent_trees.txt',sent_trees)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to examine the parse tree generated stanford corenlp annote function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing sentences with beginning of type :    (X\n",
      "4181 : At least two Nato killed in Afghanistan helicopter crash: military\n",
      "4201 : The murky waters of the South China Sea\n",
      "4580 : More French soldiers to Central African Republic\n",
      "5082 : Nearly nine million people in 'severe debt'\n",
      "5570 : Qatar's emir hands power to his son\n",
      "5607 : German ambassador's Athens residence shot at\n",
      "6417 : Q: What is the reason for people to implicitly trust their peers in extreme (or not) situations?\n",
      "7218 : Colorado Governor Visits School Shooting Victim\n",
      "10645 : **smooches Ran** Hey, Blue-Eyes, How's things in London?\n",
      "\n",
      "11069 : Specifically Gnostic.\n",
      "\n",
      "12657 : Mali's Interim President Sworn Into Office\n",
      "\n",
      "12814 : Russian air force's 100th anniversary\n",
      "\n",
      "13618 : Zimbabwe‚Äôs opposition MDC party challenges Robert Mugabe‚Äôs election win\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check sentences where second line of tree is not '  (S'\n",
    "# Checked : first line is always 'ROOT(\\n'\n",
    "# A tree has at most 160 lines\n",
    "# Note str.split('\\n') function deletes the \\n ending\n",
    "\n",
    "secondline = '  (S'             \n",
    "def styp_truncate(s):\n",
    "    l = len(s)\n",
    "    i = 3\n",
    "    ans = s[0:3]\n",
    "    try:\n",
    "        nextbrac_idx = s[3:].index('(')+3\n",
    "        return s[:nextbrac_idx+1]\n",
    "    except:\n",
    "        return s\n",
    "\n",
    "def get_nonsenttypes(trees):\n",
    "    count = 0\n",
    "    senttypes = []\n",
    "    senttypes_count = []\n",
    "    senttypes_idx = []\n",
    "    for i,tree in enumerate(trees):\n",
    "        lines = tree.split('\\n')\n",
    "        if lines[1] != secondline:\n",
    "            count = count + 1\n",
    "            styp = lines[1]\n",
    "            # if styp is more than 2 open brackets, truncate till next open bracket, else unchange\n",
    "            styp = styp_truncate(styp)\n",
    "            if styp not in senttypes:\n",
    "                senttypes.append(styp)\n",
    "                senttypes_count.append(1)\n",
    "                senttypes_idx.append([i])\n",
    "            else:\n",
    "                senttypes_count[senttypes.index(styp)] += 1\n",
    "                senttypes_idx[senttypes.index(styp)].append(i)\n",
    "    return count, senttypes, senttypes_count, senttypes_idx\n",
    "\n",
    "\n",
    "def print_sent(styp, senttypes, senttypes_count, senttypes_idx, trees, sents):\n",
    "    print(\"Printing sentences with beginning of type : \", styp)\n",
    "    try:\n",
    "        i = senttypes.index(styp)\n",
    "        for j in senttypes_idx[i]:\n",
    "            print(j,\":\",sents[j])\n",
    "            #print(trees[j])\n",
    "    except:\n",
    "        print(styp,\"is not in the list\")\n",
    "\n",
    "\n",
    "count, senttypes, senttypes_count, senttypes_idx = get_nonsenttypes(sent_trees)\n",
    "print(sum(senttypes_count))\n",
    "print(\"Atypical sentence count     : %d, %.1f%%\" %(count,count/n_data/2*100))\n",
    "print(\"Types of atypical sentences :\",len(senttypes))\n",
    "for i in range(len(senttypes)):\n",
    "    print(senttypes_count[i],\":\", senttypes[i])\n",
    "\n",
    "\n",
    "styp = '  (X'\n",
    "print_sent(styp, senttypes, senttypes_count, senttypes_idx, (sent_trees), sent_all)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove non utf-8 sequences in text for stanford corenlp\n",
    "\n",
    "Code is no longer necessary because only 1 offensive character \\x12 (') found in sentence 1 #2918 for entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences with sequence [bytearray(b'\\xc2\\xa3'), bytearray(b'\\n')] :\n",
      "----------------\n",
      "Sample  2505\n",
      "b'The festival kicked off yesterday one day after the Competition Commission delivered its final verdict to the Government on the proposed \\xc2\\xa34.1 billion merger.'\n",
      "b'The Competition Commission delivered its verdict yesterday on the proposed merger of the two big ITV players, Carlton and Granada.\\n'\n",
      "----------------\n",
      "Sample  6848\n",
      "b'UK spends \\xc2\\xa33m on foreign prisons'\n",
      "b'US spends $50m on carp invasion\\n'\n",
      "----------------\n",
      "157 157\n",
      "b'The festival kicked off yesterday one day after the Competition Commission delivered its final verdict to the Government on the proposed  4.1 billion merger.'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# extract non-utf sequences\n",
    "\n",
    "def extract_non_utf8seq(instr):\n",
    "    bstr = bytearray(instr, encoding='utf-8')\n",
    "\n",
    "    ex_utf_idx = [i for i,c in enumerate(bstr) if c in ex_utf]\n",
    "    seq_list=[]\n",
    "    if len(ex_utf_idx) >0:\n",
    "        lasti = ex_utf_idx[0]\n",
    "        seq = bytearray('',encoding='utf-8')\n",
    "        seq.append(bstr[lasti])\n",
    "        for i in ex_utf_idx[1:]:\n",
    "            if i==(lasti+1):\n",
    "                seq.append(bstr[i])\n",
    "            else:\n",
    "                seq_list.append(seq)\n",
    "                seq = bytearray('',encoding='utf-8')\n",
    "                seq.append(bstr[lasti])\n",
    "            lasti = i\n",
    "        seq_list.append(seq)\n",
    "    return seq_list\n",
    "\n",
    "badsent_idx=[]\n",
    "badsent_nonutf=[]\n",
    "for i,s in enumerate(sent1_all):\n",
    "    if notutf8(s) or notutf8(sent2_all[i]):\n",
    "        badsent_idx.append(i)\n",
    "        badsent_nonutf.append([extract_non_utf8seq(s) + extract_non_utf8seq(sent2_all[i])])\n",
    "print(\"Number of samples with non-utf8 sequences :\", len(badsent_idx))\n",
    "\n",
    "nonutf_seq_npy = np.asarray(flatten(badsent_nonutf))\n",
    "nonutf_unique = np.unique(nonutf_seq_npy)\n",
    "print(\"Number of unique nonutf sequences :\", nonutf_unique.shape[0])\n",
    "for c in nonutf_unique:\n",
    "    print(c)\n",
    "\n",
    "\n",
    "# Print sentences with specific non-utf sequences\n",
    "\n",
    "n = 2\n",
    "seqlist = nonutf_unique[n]\n",
    "print(\"Sentences with sequence\",seqlist,\":\")\n",
    "for i, nonutf_list in enumerate(badsent_nonutf):\n",
    "    if seqlist in nonutf_list:\n",
    "        print(\"----------------\")\n",
    "        print(\"Sample \",i)\n",
    "        bprint(sent1_all[badsent_idx[i]])\n",
    "        bprint(sent2_all[badsent_idx[i]])\n",
    "\n",
    "print(\"----------------\")\n",
    "m=2505\n",
    "snew = sent1_all[m].replace('\\xa3',' ')\n",
    "print(len(snew),len(sent1_all[m]))\n",
    "bprint(snew)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
